<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Using an Optimizer - AIDE-QC</title><meta name=description content="Advancing Integrated Development Environments for Quantum Computing"><meta name=generator content="Hugo 0.76.5"><link href=https://aide-qc.github.io/deployindex.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://aide-qc.github.io/deploy/users/using_optimizer/><link rel=stylesheet href=https://aide-qc.github.io/deploy/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://aide-qc.github.io/deploy/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://aide-qc.github.io/deploy/js/bundle.js></script><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://aide-qc.github.io/deploy/aide_qc_logo_v3.png width=40px align=absmiddle>
AIDE-QC</div></h1><p class=description>Advancing Integrated Development Environments for Quantum Computing</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://aideqc.slack.com>AIDE-QC Slack</a></li><li class=child><a href=https://xacc-dev.slack.com>XACC/QCOR Slack</a></li></ul></li><li class=parent><a href=https://github.com/aide-qc>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://ornl-qci.github.io/qcor-api-docs/>QCOR Doxygen</a></li><li class=child><a href=https://ornl-qci.github.io/xacc-api-docs/>XACC Doxygen</a></li><li class=child><a href=https://github.com/aide-qc/qcor>QCOR</a></li><li class=child><a href=https://github.com/aide-qc/xacc>XACC</a></li></ul></li><li><a href=https://github.com/aide-qc/aide-qc/issues>Bugs</a></li><li><a href=http://aide-qc.org>Project Page</a></li></ul></nav></div><div class=content-container><main><h1>Using an Optimizer</h1><p><code>Optimizers</code> have proven ubiquitous across variational quantum computation. AIDE-QC is focused on the development of novel
optimization strategies for noisy quantum co-processors. To deploy these strategies in an extensible and modular way, the AIDE-QC
software stack puts forward and <code>Optimizer</code> interface / concept that can be implemented for particular optimization strategies. Currently,
we provide a number of implementations, some of which delegate to popular optimization libraries that provide a wealth of gradient-based
and derivative free algorithms.</p><p>Here we describe how to use AIDE-QC <code>Optimizers</code>. We provide implementations that are backed by
<a href=https://ensmallen.org/docs.html>MLPack</a>
and
<a href=https://nlopt.readthedocs.io/en/latest/>NLOpt</a>
. If you are interested in
creating a custom <code>Optimizer</code>, checkout the
<a href=/deploy/developers/implement_optimizer/><code>Optimizer</code> Developer Guide</a>
.</p><p>For background on the <code>Optimizer</code> class itself, checkout this
<a href=/deploy/developers/implement_optimizer/#background>link</a>
.</p><h2 id=a-idcreate-optimizera-create-an-optimizer><a id=create-optimizer></a>Create an Optimizer&nbsp;<a class=headline-hash href=#a-idcreate-optimizera-create-an-optimizer>¶</a></h2><p>The <code>qcor</code> runtime library puts forward a simple <code>createOptimizer()</code> API call that will return an instance of the <code>Optimizer</code> interface.
Programmers can use this API call in both C++ and Python in order retrieve a specific desired <code>Optimizer</code> implementation with additional
custom input parameters. Here we demonstrate a few examples of this in C++ and Python</p><table><tr><th>Get the NLOpt Optimizer - C++</th><th>Get the NLOpt Optimizer - Python</th></tr><tr><td><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp>  <span class=c1>// Create the simplest Optimizer. Default 
</span><span class=c1></span>  <span class=c1>// NLopt algorithm is COBYLA
</span><span class=c1></span>  <span class=k>auto</span> <span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s>&#34;nlopt&#34;</span><span class=p>);</span>

  <span class=c1>// Get NLOpt, but use L-BFGS
</span><span class=c1></span>  <span class=k>auto</span> <span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s>&#34;nlopt&#34;</span><span class=p>,</span> <span class=p>{{</span><span class=s>&#34;algorithm&#34;</span><span class=p>,</span> <span class=s>&#34;l-bfgs&#34;</span><span class=p>}});</span>

  <span class=c1>// Get NLOpt, specify max function evaluations
</span><span class=c1></span>  <span class=k>auto</span> <span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s>&#34;nlopt&#34;</span><span class=p>,</span> <span class=p>{{</span><span class=s>&#34;algorithm&#34;</span><span class=p>,</span> <span class=s>&#34;l-bfgs&#34;</span><span class=p>},</span> <span class=p>{</span><span class=s>&#34;nlopt-maxeval&#34;</span><span class=p>,</span> <span class=mi>100</span><span class=p>}});</span>

  <span class=c1>// Get the MLPack optimizer, default is adam
</span><span class=c1></span>  <span class=k>auto</span> <span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s>&#34;mlpack&#34;</span><span class=p>);</span>

  <span class=c1>// Get MLPack, Stochastic Gradient Descent 
</span><span class=c1></span>  <span class=c1>// with custom step-size
</span><span class=c1></span>  <span class=k>auto</span> <span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s>&#34;mlpack&#34;</span><span class=p>,</span> <span class=p>{{</span><span class=s>&#34;algorithm&#34;</span><span class=p>,</span> <span class=s>&#34;sgd&#34;</span><span class=p>},</span> <span class=p>{</span><span class=s>&#34;mlpack-step-size&#34;</span><span class=p>,</span> <span class=mf>.3</span><span class=p>}});</span>
</code></pre></div></td><td><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Create the simplest Optimizer. Default </span>
<span class=c1># NLopt algorithm is COBYLA</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s1>&#39;nlopt&#39;</span><span class=p>)</span>

<span class=c1># Get NLOpt, but use L-BFGS</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s1>&#39;nlopt&#39;</span><span class=p>,</span> <span class=p>{</span><span class=s1>&#39;algorithm&#39;</span><span class=p>:</span> <span class=s1>&#39;l-bfgs&#39;</span><span class=p>})</span>

<span class=c1># Get NLOpt, specify max function evaluations</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s1>&#39;nlopt&#39;</span><span class=p>,</span> <span class=p>{</span><span class=s1>&#39;algorithm&#39;</span><span class=p>:</span> <span class=s1>&#39;l-bfgs&#39;</span><span class=p>,</span> <span class=s1>&#39;nlopt-maxeval&#39;</span><span class=p>:</span> <span class=mi>100</span><span class=p>})</span>

<span class=c1># Get the MLPack optimizer, default is adam</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s1>&#39;mlpack&#39;</span><span class=p>)</span>

<span class=c1># Get MLPack, Stochastic Gradient Descent </span>
<span class=c1># with custom step-size</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s1>&#39;mlpack&#39;</span><span class=p>,</span> <span class=p>{</span><span class=s1>&#39;algorithm&#39;</span><span class=p>:</span> <span class=s1>&#39;sgd&#39;</span><span class=p>,</span> <span class=s1>&#39;mlpack-step-size&#39;</span><span class=p>:</span> <span class=o>.</span><span class=mi>3</span><span class=p>})</span>
</code></pre></div></td></tr></table><p>Any <code>Optimizer</code> can be created and specified in this way. See below for the list of
<a href=#available-optimizers>Available Optimizers</a>
, and note
the creation of new <code>Optimizers</code> is an open research and development activity, so this list will grow over time.</p><h2 id=a-idoptimize-functiona-optimize-a-custom-function><a id=optimize-function></a>Optimize a Custom Function&nbsp;<a class=headline-hash href=#a-idoptimize-functiona-optimize-a-custom-function>¶</a></h2><p>Here we demonstrate how one might use the <code>Optimizer</code> interface to find the
optimal value and optimal parameters for a general function. To do so, we
take the problem of defining a parameterized quantum kernel, observing the
resultant state based on a custom Hamiltonian, and computing the resultant expectation
value of that observable at the given set of state input parameters. Our goal will be to define
a function to optimize that takes a list or vector of floats and returns the corresponding
expectation value.</p><p>We first demonstrate this in Python, see below:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>qcor</span> <span class=kn>import</span> <span class=o>*</span>

<span class=c1># Define an observable</span>
<span class=n>H</span> <span class=o>=</span> <span class=o>-</span><span class=mf>2.1433</span> <span class=o>*</span> <span class=n>X</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>*</span> <span class=n>X</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=mf>2.1433</span> <span class=o>*</span> \
    <span class=n>Y</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>*</span> <span class=n>Y</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>+</span> <span class=o>.</span><span class=mi>21829</span> <span class=o>*</span> <span class=n>Z</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>-</span> <span class=mf>6.125</span> <span class=o>*</span> <span class=n>Z</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>+</span> <span class=mf>5.907</span>

<span class=c1># Define a parameterized quantum kernel</span>
<span class=nd>@qjit</span>
<span class=k>def</span> <span class=nf>ansatz</span><span class=p>(</span><span class=n>q</span> <span class=p>:</span> <span class=n>qreg</span><span class=p>,</span> <span class=n>theta</span> <span class=p>:</span> <span class=nb>float</span><span class=p>):</span>
    <span class=n>X</span><span class=p>(</span><span class=n>q</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
    <span class=n>Ry</span><span class=p>(</span><span class=n>q</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>theta</span><span class=p>)</span>
    <span class=n>CX</span><span class=p>(</span><span class=n>q</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>q</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>

<span class=c1># We know the target energy</span>
<span class=n>target_energy</span> <span class=o>=</span> <span class=o>-</span><span class=mf>1.74</span>

<span class=c1># Define a function to optimize</span>
<span class=k>def</span> <span class=nf>objective_function</span><span class=p>(</span><span class=n>x</span> <span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>float</span><span class=p>]):</span>
    <span class=c1># Allocate some qubits to execute on</span>
    <span class=n>q</span> <span class=o>=</span> <span class=n>qalloc</span><span class=p>(</span><span class=n>H</span><span class=o>.</span><span class=n>nBits</span><span class=p>())</span>
    <span class=c1># Observe the ansatz at the given arguments</span>
    <span class=n>energy</span> <span class=o>=</span> <span class=n>ansatz</span><span class=o>.</span><span class=n>observe</span><span class=p>(</span><span class=n>H</span><span class=p>,</span> <span class=n>q</span><span class=p>,</span> <span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
    <span class=c1># We want to see how far we are from the target</span>
    <span class=k>return</span> <span class=nb>abs</span><span class=p>(</span><span class=n>target_energy</span> <span class=o>-</span> <span class=n>energy</span><span class=p>)</span>

<span class=c1># Create an Optimizer, NLOpt COBYLA with max of 20 function evals</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s1>&#39;nlopt&#39;</span><span class=p>,</span> <span class=p>{</span><span class=s1>&#39;nlopt-maxeval&#39;</span><span class=p>:</span><span class=mi>20</span><span class=p>})</span>

<span class=c1># Optimize!</span>
<span class=n>opt_val</span><span class=p>,</span> <span class=n>opt_params</span> <span class=o>=</span> <span class=n>optimizer</span><span class=o>.</span><span class=n>optimize</span><span class=p>(</span><span class=n>objective_function</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=k>print</span><span class=p>(</span><span class=n>opt_val</span><span class=p>,</span> <span class=n>opt_params</span><span class=p>)</span>
</code></pre></div><p>This example starts by defining a custom observable (Hamiltonian) and a parameterized
quantum kernel ansatz. The function we want to optimize observes this ansatz based on
the Hamiltonian and current input parameters, and returns the expectation value of the
Hamiltonian (the energy in this case). Our goal is to find <code>x[0]</code> that gets us as close
to the target energy as possible, so we return <code>abs(target_energy - energy)</code>. Optimizing this
function with the AIDE-QC stack is simple - create the desired <code>Optimizer</code> and call <code>optimize</code>,
providing the function to optimize and the number of variational parameters.</p><p>We can do the same thing in C++:</p><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp><span class=c1>// Define the quantum kernel ansatz
</span><span class=c1></span><span class=n>__qpu__</span> <span class=kt>void</span> <span class=nf>ansatz</span><span class=p>(</span><span class=n>qreg</span> <span class=n>q</span><span class=p>,</span> <span class=kt>double</span> <span class=n>theta</span><span class=p>)</span> <span class=p>{</span>
  <span class=n>X</span><span class=p>(</span><span class=n>q</span><span class=p>[</span><span class=mi>0</span><span class=p>]);</span>
  <span class=n>Ry</span><span class=p>(</span><span class=n>q</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>theta</span><span class=p>);</span>
  <span class=n>CX</span><span class=p>(</span><span class=n>q</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>q</span><span class=p>[</span><span class=mi>0</span><span class=p>]);</span>
<span class=p>}</span>

<span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
  <span class=c1>// Define an observable
</span><span class=c1></span>  <span class=k>auto</span> <span class=n>H</span> <span class=o>=</span> <span class=o>-</span><span class=mf>2.1433</span> <span class=o>*</span> <span class=n>X</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>*</span> <span class=n>X</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=mf>2.1433</span> <span class=o>*</span> <span class=n>Y</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>*</span> <span class=n>Y</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>+</span> <span class=mf>.21829</span> <span class=o>*</span> <span class=n>Z</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>-</span>
           <span class=mf>6.125</span> <span class=o>*</span> <span class=n>Z</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>+</span> <span class=mf>5.907</span><span class=p>;</span>

  <span class=c1>// We know the target energy 
</span><span class=c1></span>  <span class=kt>double</span> <span class=n>target_energy</span> <span class=o>=</span> <span class=o>-</span><span class=mf>1.74</span><span class=p>;</span>

  <span class=c1>// Define a function to optimize
</span><span class=c1></span>  <span class=n>OptFunction</span> <span class=n>objective_function</span><span class=p>(</span>
      <span class=p>[</span><span class=o>&amp;</span><span class=p>](</span><span class=k>const</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>double</span><span class=o>&gt;&amp;</span> <span class=n>x</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>double</span><span class=o>&gt;&amp;</span> <span class=n>gradx</span><span class=p>)</span> <span class=p>{</span>
        <span class=c1>// Get the energy, &lt;ansatz(theta) | H | ansatz(theta)&gt;
</span><span class=c1></span>        <span class=k>auto</span> <span class=n>energy</span> <span class=o>=</span> <span class=n>observe</span><span class=p>(</span><span class=n>ansatz</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>qalloc</span><span class=p>(</span><span class=n>H</span><span class=p>.</span><span class=n>nBits</span><span class=p>()),</span> <span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>]);</span>
        <span class=c1>// We want to see how far we are from the target
</span><span class=c1></span>        <span class=k>return</span> <span class=n>std</span><span class=o>::</span><span class=n>fabs</span><span class=p>(</span><span class=n>target_energy</span> <span class=o>-</span> <span class=n>energy</span><span class=p>);</span>
      <span class=p>},</span>
      <span class=mi>1</span><span class=p>);</span>

  <span class=c1>// Create an Optimizer, NLOpt COBYLA with max of 20 function evals
</span><span class=c1></span>  <span class=k>auto</span> <span class=n>optimizer</span> <span class=o>=</span> <span class=n>createOptimizer</span><span class=p>(</span><span class=s>&#34;nlopt&#34;</span><span class=p>,</span> <span class=p>{{</span><span class=s>&#34;nlopt-maxeval&#34;</span><span class=p>,</span> <span class=mi>20</span><span class=p>}});</span>

  <span class=c1>// Optimize!
</span><span class=c1></span>  <span class=k>auto</span> <span class=p>[</span><span class=n>opt_val</span><span class=p>,</span> <span class=n>opt_params</span><span class=p>]</span> <span class=o>=</span> <span class=n>optimizer</span><span class=o>-&gt;</span><span class=n>optimize</span><span class=p>(</span><span class=n>objective_function</span><span class=p>);</span>
  <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>opt_val</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>;</span>
  <span class=k>for</span> <span class=p>(</span><span class=k>auto</span> <span class=nl>x</span> <span class=p>:</span> <span class=n>opt_params</span><span class=p>)</span> <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>x</span> <span class=o>&lt;&lt;</span> <span class=s>&#34; &#34;</span><span class=p>;</span>
  <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
<span class=p>}</span>
</code></pre></div><h2 id=a-idavailable-optimizersa-available-optimizers><a id=available-optimizers></a>Available Optimizers&nbsp;<a class=headline-hash href=#a-idavailable-optimizersa-available-optimizers>¶</a></h2><p>Here we provide detailed specifications of the available <code>Optimizer</code> implementations we provide as part of the AIDE-QC software stack.
For each, we detail its available options - their key name, default values, and required types.</p><h3 id=a-idavail-mlpacka-mlpack><a id=avail-mlpack></a>mlpack&nbsp;<a class=headline-hash href=#a-idavail-mlpacka-mlpack>¶</a></h3><p>Get reference to this <code>Optimizer</code> with <code>createOptimizer("mlpack")</code>. Get reference to the various optimization strategies with
<code>createOptimizer("mlpack", {{"algorithm", "adadelta"}})</code> (<code>adadelta</code> as an example). See below for all strategies and associated
options.</p><table><thead><tr><th><code>algorithm</code></th><th>Optimizer Parameter</th><th>Parameter Description</th><th>default</th><th>type</th></tr></thead><tbody><tr><td>adam</td><td>mlpack-step-size</td><td>Step size for each iteration.</td><td>.5</td><td>double</td></tr><tr><td></td><td>mlpack-beta1</td><td>Exponential decay rate for the first moment estimates.</td><td>.7</td><td>double</td></tr><tr><td></td><td>mlpack-beta2</td><td>Exponential decay rate for the weighted infinity norm estimates.</td><td>.999</td><td>double</td></tr><tr><td></td><td>mlpack-max-iter</td><td>Maximum number of iterations allowed</td><td>500000</td><td>int</td></tr><tr><td></td><td>mlpack-tolerance</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-4</td><td>double</td></tr><tr><td></td><td>mlpack-eps</td><td>Value used to initialize the mean squared gradient parameter.</td><td>1e-8</td><td>double</td></tr><tr><td>l-bfgs</td><td>None</td><td></td><td></td><td></td></tr><tr><td>adagrad</td><td>mlpack-step-size</td><td>Step size for each iteration.</td><td>.5</td><td>double</td></tr><tr><td></td><td>mlpack-max-iter</td><td>Maximum number of iterations allowed</td><td>500000</td><td>int</td></tr><tr><td></td><td>mlpack-tolerance</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-4</td><td>double</td></tr><tr><td></td><td>mlpack-eps</td><td>Value used to initialize the mean squared gradient parameter.</td><td>1e-8</td><td>double</td></tr><tr><td>adadelta</td><td>mlpack-step-size</td><td>Step size for each iteration.</td><td>.5</td><td>double</td></tr><tr><td></td><td>mlpack-max-iter</td><td>Maximum number of iterations allowed</td><td>500000</td><td>int</td></tr><tr><td></td><td>mlpack-tolerance</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-4</td><td>double</td></tr><tr><td></td><td>mlpack-eps</td><td>Value used to initialize the mean squared gradient parameter.</td><td>1e-8</td><td>double</td></tr><tr><td></td><td>mlpack-rho</td><td>Smoothing constant.</td><td>.95</td><td>double</td></tr><tr><td>cmaes</td><td>mlpack-cmaes-lambda</td><td>The population size.</td><td>0</td><td>int</td></tr><tr><td></td><td>mlpack-cmaes-upper-bound</td><td>Upper bound of decision variables.</td><td>10.</td><td>duoble</td></tr><tr><td></td><td>mlpack-cmaes-lower-bound</td><td>Lower bound of decision variables.</td><td>-10.0</td><td>double</td></tr><tr><td></td><td>mlpack-max-iter</td><td>Maximum number of iterations allowed</td><td>500000</td><td>int</td></tr><tr><td></td><td>mlpack-tolerance</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-4</td><td>double</td></tr><tr><td>gd</td><td>mlpack-step-size</td><td>Step size for each iteration.</td><td>.5</td><td>double</td></tr><tr><td></td><td>mlpack-max-iter</td><td>Maximum number of iterations allowed</td><td>500000</td><td>int</td></tr><tr><td></td><td>mlpack-tolerance</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-4</td><td>double</td></tr><tr><td>momentum-sgd</td><td>mlpack-step-size</td><td>Step size for each iteration.</td><td>.5</td><td>double</td></tr><tr><td></td><td>mlpack-max-iter</td><td>Maximum number of iterations allowed</td><td>500000</td><td>int</td></tr><tr><td></td><td>mlpack-tolerance</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-4</td><td>double</td></tr><tr><td></td><td>mlpack-momentum</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>.05</td><td>double</td></tr><tr><td>momentum-nesterov</td><td>mlpack-step-size</td><td>Step size for each iteration.</td><td>.5</td><td>double</td></tr><tr><td></td><td>mlpack-max-iter</td><td>Maximum number of iterations allowed</td><td>500000</td><td>int</td></tr><tr><td></td><td>mlpack-tolerance</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-4</td><td>double</td></tr><tr><td></td><td>mlpack-momentum</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>.05</td><td>double</td></tr><tr><td>sgd</td><td>mlpack-step-size</td><td>Step size for each iteration.</td><td>.5</td><td>double</td></tr><tr><td></td><td>mlpack-max-iter</td><td>Maximum number of iterations allowed</td><td>500000</td><td>int</td></tr><tr><td></td><td>mlpack-tolerance</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-4</td><td>double</td></tr><tr><td>rms-prop</td><td>mlpack-step-size</td><td>Step size for each iteration.</td><td>.5</td><td>double</td></tr><tr><td></td><td>mlpack-max-iter</td><td>Maximum number of iterations allowed</td><td>500000</td><td>int</td></tr><tr><td></td><td>mlpack-tolerance</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-4</td><td>double</td></tr><tr><td></td><td>mlpack-alpha</td><td>Smoothing constant</td><td>.99</td><td>double</td></tr><tr><td></td><td>mlpack-eps</td><td>Value used to initialize the mean squared gradient parameter.</td><td>1e-8</td><td>double</td></tr></tbody></table><h3 id=a-idavail-nlopta-nlopt><a id=avail-nlopt></a>nlopt&nbsp;<a class=headline-hash href=#a-idavail-nlopta-nlopt>¶</a></h3><p>Get reference to this <code>Optimizer</code> with <code>createOptimizer("nlopt")</code>. Get reference to the various optimization strategies with
<code>createOptimizer("nlopt", {{"algorithm", "l-bfgs"}})</code> (<code>l-bfgs</code> as an example). See below for all strategies and associated
options.</p><table><thead><tr><th><code>algorithm</code></th><th>Optimizer Parameter</th><th>Parameter Description</th><th>default</th><th>type</th></tr></thead><tbody><tr><td>cobyla</td><td>nlopt-ftol</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-6</td><td>double</td></tr><tr><td></td><td>nlopt-maxeval</td><td>Maximum number of iterations allowed</td><td>1000</td><td>int</td></tr><tr><td>l-bfgs</td><td>nlopt-ftol</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-6</td><td>double</td></tr><tr><td></td><td>nlopt-maxeval</td><td>Maximum number of iterations allowed</td><td>1000</td><td>int</td></tr><tr><td>nelder-mead</td><td>nlopt-ftol</td><td>Maximum absolute tolerance to terminate algorithm.</td><td>1e-6</td><td>double</td></tr><tr><td></td><td>nlopt-maxeval</td><td>Maximum number of iterations allowed</td><td>1000</td><td>int</td></tr></tbody></table><h3 id=a-idscikitquanta-scikit-quant><a id=scikitquant></a>scikit-quant&nbsp;<a class=headline-hash href=#a-idscikitquanta-scikit-quant>¶</a></h3><p>Get reference to this <code>Optimizer</code> with <code>createOptimizer("skquant")</code>. Get reference to the various optimization strategies with
<code>createOptimizer("skquant", {{"method", "imfil"}})</code> (<code>imfil</code> as an example). See below for all strategies and associated
options.</p><p>To use this <code>Optimizer</code> you must install it separately:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>python3 -m pip install --user scikit-quant
</code></pre></div><table><thead><tr><th><code>algorithm</code></th><th>Optimizer Parameter</th><th>Parameter Description</th><th>default</th><th>type</th></tr></thead><tbody><tr><td>(all)</td><td>budget</td><td>Number of allowed function evaluations.</td><td>100</td><td>int</td></tr></tbody></table><h3 id=a-idlibcmaesa-libcmaes><a id=libcmaes></a>libcmaes&nbsp;<a class=headline-hash href=#a-idlibcmaesa-libcmaes>¶</a></h3><p>Get reference to this <code>Optimizer</code> with <code>createOptimizer("cmaes")</code>. Get reference to the various optimization strategies with
<code>createOptimizer("cmaes", {{"cmaes-max-iter", 100}})</code> (<code>cmaes-max-iter</code> as an example). See below for all strategies and associated
options.</p><p>To use this <code>Optimizer</code> you must install it separately:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>git clone https://github.com/ornl-qci/libcmaes
<span class=nb>cd</span> libcmaes <span class=o>&amp;&amp;</span> mkdir build <span class=o>&amp;&amp;</span> <span class=nb>cd</span> build
cmake .. -DXACC_DIR<span class=o>=</span><span class=k>$(</span>qcor -xacc-install<span class=k>)</span> -DEIGEN3_INCLUDE_DIR<span class=o>=</span><span class=k>$(</span>qcor -xacc-install<span class=k>)</span>/include/eigen -DCMAKE_INSTALL_PREFIX<span class=o>=</span><span class=nv>$HOME</span>/.libcmaes
make install
</code></pre></div><table><thead><tr><th>Optimizer Parameter</th><th>Parameter Description</th><th>default</th><th>type</th></tr></thead><tbody><tr><td>cmaes-max-iter</td><td>Number of allowed cmaes iterations.</td><td>-1</td><td>int</td></tr><tr><td>cmaes-max-feval</td><td>Number of allowed function evaluations.</td><td>-1</td><td>int</td></tr></tbody></table><div class=edit-meta><br><a href=https://github.com/aide-qc/deploy//edit/master/website/content/users/using_optimizer.md class=edit-page><i class="fas fa-pen-square"></i>Edit on GitHub</a></div><nav class=pagination><a class="nav nav-prev" href=/deploy/users/tnqvm/ title="Tensor Network Quantum Virtual Machine"><i class="fas fa-arrow-left" aria-hidden=true></i>Prev - Tensor Network Quantum Virtual Machine</a>
<a class="nav nav-next" href=/deploy/users/variational/ title="Variational Algorithms">Next - Variational Algorithms <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://aide-qc.github.io/deploy>Home</a></li><li class=has-sub-menu><a href=/deploy/background/>Background<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/deploy/background/qcor/>QCOR C++ Compiler and JIT Engine</a></li><li><a href=/deploy/background/xacc/>XACC Quantum Programming Framework</a></li><li><a href=/deploy/background/project/></a></li></ul></li><li class=has-sub-menu><a href=/deploy/developers/>Developer Guide<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/deploy/developers/implement_circuit_synthesis/>Add a New Circuit Synthesis Strategy</a></li><li><a href=/deploy/developers/implement_optimizer/>Add a New Optimizer</a></li><li><a href=/deploy/developers/implement_qsim_workflow/>Add a New QSim Workflow</a></li><li><a href=/deploy/developers/implement_accelerator/>Add a New Quantum Backend</a></li><li><a href=/deploy/developers/implement_plugin_embedded_python/>Hybrid C++ / Python Plugins</a></li><li><a href=/deploy/developers/clang_syntax/></a></li></ul></li><li class=has-sub-menu><a href=/deploy/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu></ul></li><li class="parent has-sub-menu"><a href=/deploy/users/>User Guide<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=/deploy/users/hello_world/>Hello World</a></li><li><a href=/deploy/users/operators/>Operators</a></li><li><a href=/deploy/users/pass_manager/>Pass Manager</a></li><li><a href=/deploy/users/qjit/>Quantum JIT (QJIT)</a></li><li><a href=/deploy/users/quantum_kernels/>Quantum Kernels</a></li><li><a href=/deploy/users/qsim/>Quantum Simulation (QSim)</a></li><li><a href=/deploy/users/remote_qpu_creds/>Remote QPU Credentials</a></li><li><a href=/deploy/users/tnqvm/>Tensor Network Quantum Virtual Machine</a></li><li class=active><a href=/deploy/users/using_optimizer/>Using an Optimizer</a></li><li><a href=/deploy/users/variational/>Variational Algorithms</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>